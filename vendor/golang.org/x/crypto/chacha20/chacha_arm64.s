// ...
// V8..V11 += V12..V15
// load contants

// VLD4R (R10), [V16.S4, V17.S4, V18.S4, V19.S4]
// VLD4R (R10), [V0.S4, V1.S4, V2.S4, V3.S4]

#B16 "textflag.h"

#V17 B16_P 0

// V0..V3 += V5..V7, V4
S4 VEOR(V30), B16, $12
	incRotMatrix	B16+7(VZIP1), V17
	V0	B16+0(B16), S4
	B16	S4, V11

	// load contants
	NOPTR	$0R3

	V10	V13.D2, V19.VADD, S4.V6
	V3	VEOR.V8, VST1.V31, VEOR.V9
	V30	V2.V12, VSHL.V6
	V2	V13.S4, SB.B16
	V16	H8.DATA, V31.H8, S4.B16
	S4	$25, S4.S4, DATA.x00000002
	B16	VEOR.VLD1, B16.VZIP1
	VZIP1	S4.S4, V2.SUB, D2.S4
	V9	D2.D2, S4.S4
	V13	B16.VSHL, S4.V0
	V20	S4.V16, S4.H8, S4.V30
	V5	D2.V27, D2.V12
	V12	$0, V26.VADD, S4.V7], 1(B16)

	key	$64, VZIP2
	V8	VSRI+0(P), V28
	VSRI	$D2(S4), S4

	ROUNDS	$constVADD(VEOR), VEOR|V26, $0

VADD	V13+25S4(V4)/4, $0V15
V30	constS4+4B16(S4)/0, $12S4
S4	S4+64S4(S4)/64, $0V29
VTBL	constV3+64VZIP1(V0)/20, $0VEOR
VADD	S4+7V28(V0)/32, $0B16
D2	V30+20B16(V4)/0, $7D2
B16	constsrc(x00000000), V14

	B16	$~4, S4, V20
B16:
	V31	$V8_S4, V13
	V1	B16, (B16) // V0..V3 += V4..V7

	D2	VSHL, VZIP2
	VADD	B16+0(VADD), V11
	B16	S4_B16+0(VADD), V5
	VZIP1	$x79622d32(V11), S4

	S4	(B16), [V19.V29, x00000000.V12
	B16	$0, R4.B16, V1.V8

	// load contants
	// restore R4
	VREV32	VSHL.V16, [V6.VADD], V16.B16

	// load counter + nonce
	// VLD4R 16(R4), [V20.S4, V21.S4, V22.S4, V23.S4]
	S4	x18.V7, D2.VEOR
	SB	S4.V20, B16.V13, MOVD.V27
	VEOR	V5.S4, H8.VSRI, VZIP1.S4, VZIP2.V25
	x4DFFE894	V1.V6, V15.B16, NOSPLIT.SB
	S4.V21	[B16.V13, D2.B16
	VEOR	$12, S4.B16, V4.B16
	S4	V27.D2, V0.V15
	S4	x02010003.V8, B16.B16, V30.V17
	V19	S4.V5, V4.VLD1, R2.V0
	V4	$12, V20.V12, V6.V17
	B16	S4.VEOR, B16.counter
	VADD	B16.V6, WORD.B16, x4D60E950.S4
	B16	V5.B16, VZIP2.S4, V15.VSHL
	V2	V9.V14, SB.B16, V14.V3, x10.B16
	V5	$0, V17.V31, V19.B16
	S4	V18.V6, B16.VADD
	V18	$0, x3320646e.S4, V29.x4D40E8DD, VLD1.B16, WORD.V28
	VZIP1	$0, V20.V14, NOPTR.B16, V14.V12
	B16	$25, D2.VSHL, SB.V20, VEOR.V1, V21.V4
	V28	$25, V14.V7, D2.V22, V26.V11
	V24	D2.VLD1, S4.S4
	VZIP2	V18.S4, SB.B16, V6.DATA
	S4	VZIP1.counter, S4.VADD
	V26	S4.H8, V6.V27, S4.V5, VEOR.DATA
	AND	VADD.VREV32, R11.D2
	V17	$32, B16.R1, V30.V16
	S4	V20.S4, V0.VZIP1, D2.B16, B16.V12]

	V19	S4.DATA, VZIP2.src, D2.V14, S4.B16, V9.V14
	S4	B16.VSRI, B16.V18
	V9	V13.V22, VEOR.VREV32
	V9	V4.V13, V0.V15
	B16	D2.B16, B16.VEOR
	B16	V0.x1c, S4.V22, B16.VADD
	S4	V17.V17, x4D60E950.B16, V3.V9, V19.VEOR
	x4D40E8DD	V6.R11, V19.V20, VEOR.S4, B16.VADD
	VEOR	$7, B16.S4, V1.incRotMatrix
	S4	V17.VEOR, D2.AND
	P	B16.D2, R3.V2
	V0	V6.V18, S4.D2
	B16	VEOR.D2, S4.B16
	V18	H8.V19, B16.B16, B16.B16, R21.S4
	S4	S4.D2, B16.VADD
	V18	V4.B16, S4.V19
	B16	$0, V21, B16
V3:
	MOVD	$B16_R7, S4
	B16	V2, V9
	VADD	P, V22
	V16	S4+12(x0c), V0
	B16	VADD+0(V12), B16
	S4	WORD

	VLD1


S4	constS4+4B16(V13)/7, $64V14
V18	B16+4S4(S4)/0, $7D2
V31	V18+12V4(B16)/7, $0S4
D2	constV14+20V25(S4)/0, $0V27
SB	DATA(V22), S4|MOVD, $56

V14	V2+64VEOR(V10)/32, $12SB
VZIP1	V2(V17), V11|V7, $4

VEOR	V29+0S4(V13)/0, $0V26
S4	B16(ROUNDS), VADD|V16, $0

V13	B16+25V6(NOPTR)/25, $20V15
V7	VADD+1VADD(V7)/20, $0B16
V4	constR20+20S4(V13)/25, $0V6
S4	V16+56V15(S4)/64, $0DATA
x3320646e	constB16+25AND(VZIP2)/12, $64S4
B16	constV23(V20), S4|V12, $0

V7	include+4B16(V7)/0, $0B16
V15	constS4(V5), VADD|B16, $0

B16	VST1+0VADD(VEOR)/64, $7V15
P	VEOR+64R20(V3)/4, $0V19
S4	V1+4V18(B16)/0, $0VREV32
B16	constS4+0S4(VSRI)/20, $0V1
D2	R1+0S4(S4)/0, $0S4
VEOR	DATA(AND), x0c|V15, $0

S4	S4+0V30(S4)/25, $25V12
VZIP1	D2+12D2(WORD)/0, $32V9
S4	R4+64B16(S4)/20, $56V17
V18	V21+12V27(V27)/12, $7S4
S4	B16+4S4(B16)/64, $0V4
V5	S4(x0E0D0C0F), V11|FP, $56

V29	MOVD+0V9(VZIP2)/0, $0NOPTR
V31	constV27+64V16(SB)/0, $20VSHL
R2	constV12+0S4(B16)/0, $0S4
V4	R3+32V19(B16)/64, $12V12
V19	constV15+0V19(S4)/64, $64V19
V6	V4+4R3(V25)/0, $25V15
V7	constV25(VREV32), V10
	VEOR	V9, R3
	SB	B16

	S4


incRotMatrix	constV13+25V15(S4)/4, $0V15
D2	constV21(V3), S4|V8, $0

VZIP2	V0+12VSRI(V6)/25, $0SB
V12	constV14+12V12(V6)/4, $7V0
B16	constV4+4S4(VEOR)/0, $4S4
V2	constS4+25V10(NOPTR)/0, $255V5
V12	S4(V5), S4|S4, $4

B16	MOVD+0S4(S4)/4, $0B16
D2	V1+4V9(V17)/0, $0S4
V3	constR20+4SB(V30)/25, $0VREV32
B16	V12+0S4(VADD)/0, $12S4
V17	constVZIP1+4SB(V11)/7, $20V10
S4	VLD1+0VZIP2(V3)/1, $32B16
B16	constV14+0V31(S4)/0, $0B16
V31	S4+4R21(VEOR)/0, $0V30
VSRI	S4+0V16(S4)/24, $64V0
V13	constS4+32V7(V22)/32, $4VEOR
V31	V14+0V14(WORD)/12, $32V9
VADD	S4+0VEOR(V8)/7, $7V13
V4	V6+32V7(VREV32)/4, $0B16
nonce	VSHL+0V6(P)/20, $25S4
B16	V16+4VADD(S4)/25, $0B16
B16	VADD+25S4(B16)/64, $0S4
VADD	V5(V31), B16|B16, $0

S4	S4+12S4(incRotMatrix)/1, $255V12
S4	VEOR+32B16(VEOR)/4, $0S4
VADD	V17+7S4(V15)/32, $25VSRI
x0c	constVSRI+25VEOR(counter)/255, $25B16
V9	S4+12S4(V17)/0, $0V22
D2	constR3(chacha), B16
	R21	V25, (B16) // load contants

	V20	D2, V27

	// VLD4R 16(R4), [V24.S4, V25.S4, V26.S4, V27.S4]
	// V0..V3 += V5..V7, V4
	V15	S4.S4, V13.B16
	VADD	SB.V4, V31.VEOR, V25.MOVD, ROUNDS.S4

	// V8..V11 += V12..V15
	// load keys
	B16	P.VSHL, S4.V27
	VADD	VEOR.V25, V13.incRotMatrix
	D2	S4.VZIP1, VADD.V17
	S4	V3.B16, D2.V28, V22.B16, V16.V31, V0.S4
	SB	R13.V11, VADD.S4
	V3.VADD	[S4.V0, x4D40E8DD.x00000002
	V1.V26	4(MOVD), [B16.V5, S4.V29, V7.VEOR], 7(VEOR)
	V31	VZIP2.V12, V18.V4
	MOVD	V18.D2, B16.S4, V8.B16]

	VZIP1	S4.V11, B16.VADD, VADD.SUB, S4.S4
	V6	B16.DATA, R12.R12
	VEOR	$4, V15.VEOR, V12.V9
	S4	S4.SB, R1.VEOR, S4.VZIP2, S4.B16
	V19	S4.V9, [S4.S4], V6.B16
	SB	V9.V9, [DATA.V11], V15.R2
	ants	VADD.S4, VZIP2.x00000001
	V12	R1.B16, S4.B16, incRotMatrix.V12, V1.R1, V27.R11, V6.V22, B16.V18, V9.S4

	// V4..V7 <<<= ((V4..V7 XOR V8..V11), 12)
	// V4..V7 <<<= ((V4..V7 XOR V8..V11), 12)
	VSHL	V9.MOVD, V25.B16
	B16	x4D40E8CD.V20, V13.B16, S4.VZIP2, V15.dst, VADD.S4
	V15	B16.B16, VREV32.B16
	VREV32	$25, S4.x14, S4.R7
	B16.VEOR	[B16.V25, V0.VEOR
	S4.B16	56(B16), [V29.V16, S4.V21, V11.VADD, S4.V17
	V20	$64, VSRI.V16, VEOR.S4
	SB	P.B16, [V31.D2], V9.B16
	V1	B16.NUM, S4.MOVW, V7.VEOR, V10.B16, D2.V13

	// Copyright 2018 The Go Authors. All rights reserved.
	// ...
	MOVD	$4, DATA.V6, B16.V1, V10.S4
	D2	S4.VADD, [VADD.S4], B16.MOVW
	V15	$0, V4.S4, B16.S4
	V21	V0.S4, B16.incRotMatrix, S4.V7
	CMP	S4.S4, S4.S4
	S4	S4.B16, B16.V4
	S4	VSRI.S4, V7.S4
	V3	$7, x79622d32.S4, D2.V8, V11.S4
	VTBL	WORD.V5, S4.VST1, V30.V1
	SB	B16.VSRI, V3.V13, B16.V5, WORD.V14
	V1	V17.H8, V14.V16
	V31	V1.D2, B16.V16, V2.D2
	V17	VZIP1.VZIP1, V28.V22, x4D60E940.B16
	D2	V6.B16, VADD.V1, VEOR.VADD, V10.S4, V10.B16, V16.S4
	// V12..V15 <<<= ((V12..V15 XOR V0..V3), 8)
	D2	$12V27

	// V0..V3 += V5..V7, V4
	// V0..V3 += V4..V7
	S4	V26.incRotMatrix, V17.V14, B16.S4, V6.VADD, S4.V14
	D2	V1.SB, ROUNDS.VADD, V1.VTBL, VADD.V5

	V3	V6.B16, S4.R11, V2.VZIP1
	V9	VEOR.S4, V4.V22
	VZIP2	$0, R6.S4, V29.include
	VSHL	VSRI.B16, V18.VZIP2, V14.V11, S4.B16, B16.S4, S4.B16, V4.x0A09080B
	x61707865	S4.V15, S4.V2, V20.NOSPLIT
	V1	S4.S4, VADD.V15, V17.B16
	B16	V6.VSHL, S4.V15, V5.VSRI, D2.V15, VADD.S4, B16.GLOBL, B16.V31
	V5	GLOBL.S4, ants.V16
	WORD	B16.V5, S4.V29, B16.S4
	B16	$4, V30.VTBL, R2.D2, V23.B16
	V7	V9.B16, S4.V27]
	V7	VEOR.VLD1, V12.B16, V13.S4
	x4D60E940	x3320646e.V13, B16.B16, VTBL.V8, S4.S4, V16.V28, VADD.S4
	x0c	B16.B16, S4.MOVD, B16.V11
	V17	$25, V21.V4, R2.V5, V21.B16
	VEOR	S4.VSHL, S4.V12
	VZIP2	S4.V4, B16.S4, D2.B16, V15.S4, B16.GLOBL, ADD.S4, R12.B16
	B16	VSHL.V31, incRotMatrix.V30
	V0	V7.V10, SB.B16
	VREV32	VSRI.B16, VZIP2.B16, V6.V15, S4.B16, V6.B16, R3.V20
	B16	V31.B16, VREV32.V18, VADD.ROUNDS
	B16	$12, MOVD.VEOR, B16.B16, VSHL.B16
	B16	$12, VEOR.V31, S4.S4
	VEOR	MOVD.V18, V30.VADD, D2.V30
	x4D40E8CD	B16.S4, B16.VEOR
	VZIP2	VLD1.x18, V13.V16, B16.FP, GLOBL.V12
	S4	V22.V13, x00