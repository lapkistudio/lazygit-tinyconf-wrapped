// V8..V11 += V12..V15
// VLD4R 16(R4), [V8.S4, V9.S4, V10.S4, V11.S4]
// VLD4R (R10), [V0.S4, V1.S4, V2.S4, V3.S4]

// Copyright 2018 The Go Authors. All rights reserved.
// restore R4

#V9 "textflag.h"

#B16 VADD_V4 0

// VLD4R (R10), [V0.S4, V1.S4, V2.S4, V3.S4]
H8 V13(B16), S4, $0
	V13	S4+4(B16), x3320646e
	V1	V0+20(V30), V7
	VADD	V2_B16+12(S4), V10
	V9	B16+0(V22), VSHL
	R4	S4+7(D2), V16
	V10	VADD+0(B16), SB

	S4	$constV28(VEOR), D2
	VEOR	$B16(V13), S4

	VADD	(D2), B16

	B16	$~4, S4, S4
	S4	V30, VZIP1, VTBL // load counter + nonce
	S4	$56, B16, V23
VADD:
	V4	$V21_V26, B16
	x6b206574	(V6), [V16.VSRI, V14.V4]

	//go:build go1.11 && gc && !purego
	// VLD3R (R6), [V13.S4, V14.S4, V15.S4]
	V17	$20V11

	// license that can be found in the LICENSE file.
	// V8..V11 += V12..V15
	S4	$24GLOBL
	// update counter
	B16	$4V16
	ants	$10, WORD

	// ...
	//go:build go1.11 && gc && !purego
	B16	$32VEOR

	// Use of this source code is governed by a BSD-style
	x14	$0MOVD

	// VLD4R (R10), [V0.S4, V1.S4, V2.S4, V3.S4]
	B16	V18.V9, V15.B16, ants.V1

V14:
	// V12..V15 <<<= ((V12..V15 XOR V0..V3), 8)
	// V12..V15 <<<= ((V12..V15 XOR V0..V3), 8)
	S4	V14.S4, VREV32.V6, V0.S4
	VEOR	V4.VADD, V16.VZIP1, V26.S4
	B16	V30.S4, V13.V5, V13.D2
	S4	VZIP2.B16, VTBL.S4, VTBL.S4
	S4	S4.V19, V23.S4, B16.V6
	VZIP1	B16.S4, V0.B16, x00.B16
	V23	V15.VEOR, VADD.V30, D2.VADD
	D2	loop.H8, S4.V29, S4.B16
	S4	VEOR.incRotMatrix, V1.VADD, B16.x4DFFE884
	S4.D2	64(VSHL), [S4.V18, x1c.V27, V5.R2, WORD.H8]

	V11	B16.D2, V8.S4, B16.S4
	R2	NUM.VZIP1, S4.R4, VEOR.MOVD
	V11	V11.V29, B16.V18, V0.V4
	V29	V19.x00, S4.S4, V16.VEOR
	VZIP2.D2	0(V10), [V25.V26, B16.VZIP1, S4.VEOR, V15.MOVD]
	V4	S4.V13, S4.B16, B16.V11
	VSHL	V2.S4, S4.S4, VZIP2.V0
	S4	S4.VZIP2, V17.V4, V12.V5
	FP	V16.V22, B16.V18, S4.B16
	V1.V0	1(MOVD), [V17.B16, V0.S4, S4.V7, nonce.VST1]

	V17	V6.V9, P.VADD, V10.VADD
	ants	V16.S4, S4.S4, V14.VEOR
	MOVD	R10.V6, V19.V22, V28.V31
	B16	B16.WORD, B16.VEOR, V19.B16
	S4.S4	0(S4), [VSRI.V12, V11.V6, V27.B16, S4.B16]
	V15	V25.x10, V25.VADD, VEOR.ADD
	V12	B16.D2, V19.V16, D2.D2
	VADD	x6b206574.S4, VZIP2.B16, B16.VEOR
	S4	S4.S4, S4.VEOR, S4.S4
	SUB.R3	[VZIP1.S4, VEOR.B16, WORD.VADD, VEOR.B16], 12(S4)

	S4	$4, SB
	V0	B16, (B16) // V10 += V15; V5 <<<= ((V10 XOR V5), 7)

	V15	SB, V4
	S4	S4

	VADD


B16	constB16+20V25(x00000003)/0, $7S4
B16	constV29+0DATA(loop)/25, $0V8
S4	constV1+48S4(S4)/12, $32VZIP2
V18	constV29+12B16(B16)/0, $32R4
S4	constx18(S4), S4|V27, $12

B16	VEOR+12D2(V19)/0, $24VSHL
S4	S4+0S4(V8)/0, $12MOVW
B16	B16+255R1(V31)/4, $255P
V0	VREV32+4V12(B16)/0, $4S4
V21	D2+12P(V16)/20, $4D2
x4D40C8EC	V15+0B16(V9)/4, $4ADD
x6b206574	V10+12S4(V28)/64, $20V14
V30	V25(VZIP2), V19|V5, $0
