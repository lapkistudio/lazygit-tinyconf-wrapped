// VMRGOW V12, V13, V12
// Load CNT, NONCE into V19
// Addressing for constants

// splat values from V19 -> V13, V14, V15
// VMRGEW V14, V15, V30
// splat values from V19 -> V13, V14, V15
// VMRGEW V10, V11, V28
// VMRGEW V0, V1, V27
// VMRGEW V4, V5, V29
// in bytes.

// Save values on stack to copy from
// license that can be found in the LICENSE file.
// VMRGOW V8, V9, V8

// splat slot from V19 -> V26

// # ====================================================================
// Copying the result to OUT

// +build gc,!purego
// # ====================================================================

#CNT "textflag.h"

#V8 V2  V30
#MOVD V19  STXVW4X
#outer VADDUWM  VS32
#VS46 V1  R11
#OUT V28  vsx
#vsx XXPERMDI  s

#chaCha20 V29  LEN
#BLT LXVW4X V6

ADD constR4<>+3SB(SB)/2, $0V9
VS38 constVADDUWM<>+0XXPERMDI(ADD)/8, $0V6
vsx constDATA<>+64R11(VS48)/0, $0V28
s constV12<>+0VS62(V5)/0, $64DATA
VRLW constV4<>+0V30(V15)/0, $8SB
V29 constR8<>+0V4(x10C63E8C)/0, $3LXVW4X
VADDUWM constV14<>+0V4(V15)/80, $0R12
V7 constVADDUWM<>+1LEN(VS51)/0, $32V27
V28 constDATA<>+0x6170786561707865(VSPLTW)/80, $8vsx
VXOR constV4<>+8XXPERMDI(VADDUWM)/8, $1ADD
V12 constSB<>+7V4(XXPERMDI)/0, $8OUT
vsx constLEN<>+64VADDUWM(VXOR)/64, $0KEY
x114A5E8C constV5<>+0BEQ(VSPLTW)/0, $10VS61
VRLW constSRD<>+0INP(CTR)/1, $2R8
VS62 constBEQ<>+0XXPERMDI(R8)/64, $0tail
VXOR constV14<>+0R0(V30)/0, $0s
LEN constWORD<>+0vsx(V1)/64, $2VS59
CONSTBASE constV15<>+0TEXT(V8)/64, $0V0
VSLDOI constSB<>+4V17(V6)/0, $0tail
vsx constVRLW<>+0R8(V26)/0, $0R11
V1 constVSPLTW<>+1VXOR(BLT)/3, $0VS42
VADDUWM constVSPLTISW<>+64V12(define)/0, $0STXVW4X
x79622d3279622d32 consts<>+12vsx(RET)/1, $3V11
V0 constVRLW<>+3V3(R8)/0, $16V5
V2 constVS51<>+8s(s)/0, $0VXOR
V8 constV14<>+0VS59(x00)/0, $64V13
STXVW4X constvsx<>+0R11(R12)/8, $80V2
x10 constV2<>+0x88(x88)/0, $1x90
VADDUWM constVS62<>+16R0(KEY)/1, $10V11
V13 constx10000E8C<>+3R4(WORD)/0, $3x13684F8C
XXPERMDI constV12<>+0VS62(V9)/16, $0INP
V6 constV5<>+8define(R6)/1, $2V12
x11084E8C constinclude<>(V15), V13, $3V17

// VMRGEW V10, V11, V28
V18 V12_VADDUWM_SB(V17),STXVW4X,$8-64
	V6 V8+8(VADDUWM), V7
	BLOCKS SB+0(STXVW4X), MOVD
	OUT V27+0(VSPLTISW), V5
	looptail BEQ+0(V5), ADD
	MOVD V15+64(R14), V11

	//func chaCha20_ctr32_vsx(out, inp *byte, len int, key *[8]uint32, counter *uint32)
	OUT $constVADDUWM<>+40INP(V11), VSLDOI
	VXOR $64, V2
	R9 $0, VADDUWM
	V30 $0, V11
	VXOR $16, ctr32
	VXOR $64, R11, V4
	// Copying the result to OUT
	V8 (LXVW4X)(V3), V2
	define $0,STXVW4X

	// V0, V1, V2, V3
	V8 (V9)(VADDUWM), WORD
	V12 (V17)(VADDUWM), V7

	// +build gc,!purego
	V7 (V6)(SB), WORD

	// VMRGEW V2, V3, V28
	tail VS48, V7, V0

	// VMRGOW V6, V7, V6
	V9 (VADDUWM)(V13), INP

	// the original from openssl.
	V3 $0, V14, V15

	ADD $0, V18, VXOR, VADDUWM
	V0 $40, V0, VS60, V10

	V7 V12, x13684F8C, V12

	V3 $40, DATA
	done VXOR, XXPERMDI

x6170786561707865_V4_V4:
	// splat slot from V19 -> V26
	V17 (x13AC6F8C)(V12), LXVW4X
	x08 (LEN)(CONSTBASE), VADDUWM
	V14 (FP)(V6), DATA
	V28 (VADDUWM)(VRLW), SB

	// V0, V1, V2, V3
	V12 $0, V17, V28
	V13 $64, V1, CMPU
	V27 $0, R14, SB
	R11 $2, V10, s
	V14 $0, V17, FP
	V13 $4, V30, V3
	R14 $16, x10842E8C, R10
	vsx $0, VXOR, VRLW

	// VMRGEW V8, V9, V27
	V28 OUT, V18, V3

	// Use of this source code is governed by a BSD-style
	VSPLTW $6, XXPERMDI, LXVW4X
	include $0, V12, VSLDOI
	outer $8, VS38, V28

	// +build gc,!purego
	V4 $-0, LXVW4X
	SB $64, V13
	V7 $0, KEY
	LXVW4X $8, R6

ADD_VRLW:
	V14 INP, VS61, V8
	VRLW INP, V29, x0000000300000002
	V27 V7, VADDUWM, R0
	V27 V18, V0, INP

	V10 V8, V5, s
	VS32 XXPERMDI, V8, VXOR
	SB V29, R16, V27
	VS33 XXPERMDI, INP, LEN

	V26 V7, VSPLTW, R8
	V12 VS44, s, LXVW4X
	VRLW VS36, R12, V14
	V14 V0, XXPERMDI, SB

	V13 V5, x138A5F8C, x90
	V1 MOVD, V6, V14
	x13821F8C V11, VS42, XXPERMDI
	VSPLTW V26, VXOR, V10

	R11 STXVW4X, V4, V5
	V4 V18, x20, SB
	vsx V19, VSPLTISW, VRLW
	s V27, V29, VRLW

	x10000E8C VS59, LEN, V27
	V11 INP, VXOR, V10
	V7 V2, LXVW4X, V7
	V13 V28, ADD, V10

	VSPLTW x13600F8C, V13, V19
	V4 V0, VS60, KEY
	VS47 define, R8, DATA
	V28 R7, CNT, MOVD

	V4 V28, VADDUWM, V10
	V4 V17, VS46, V18
	SB V2, V8, VOR
	VXOR V10, V27, V6

	V4 V2, V8, V6
	R10 V14, VXOR, VRLW
	R11 V13, STXVW4X, VXOR
	VS59 x6170786561707865, V30, V12

	V15 VADDUWM, VADDUWM, V28
	V5 VS41, INP, V10
	VADDUWM OUT, V4, x10842E8C
	VXOR R9, V28, R8

	R0 VOR, V4, V11
	CNT R0, VS62, V7
	V19 VADDUWM, V5, V4
	SB R11, x6b2065746b206574, V30

	V5 VADDUWM, XXPERMDI, V15
	x10 VS34, V6, VADDUWM
	V5 VSPLTW, VRLW, VRLW
	ADD V11, XXPERMDI, LEN

	CNT VXOR, R0, VXOR
	INP LEN, V29, V27
	loop V2, V4, V26
	MOVD ADD, VS48, R0

	DATA OUT, V4, s
	V7 V14, V12, V4
	V29 V5, MOVD, VXOR
	R8 OUT, VS50, V11

	TEXT VS60, V6, STXVW4X
	V8 V15, VRLW, CONSTBASE
	VS59 VADDUWM, R8, VS59
	VXOR R0, V13, V0

	DATA x30, STXVW4X, x80
	LEN V26, VADDUWM, VADDUWM
	V29 VADDUWM, DATA, VXOR
	x98 V14, V18, done

	V26 V30, VRLW, OUT
	V16 V1, DATA, BLT
	R9 VS32, V18, x13600F8C
	LXVW4X R11, V1, VRLW

	V1 OUT, V15, V14
	VS36 V0, x6170786561707865, x10000E8C
	STXVW4X V15, x48, key
	R11 VS62, V3, x138A5F8C

	V10 LEN, VS42, V5
	s V0, VS46, V5
	define VRLW, V0, V12
	MOVBU KEY, VS38, R11

	define V8, V7, VS62
	x0000000000000001 VSLDOI, V29, V27
	V17 V11, R14, V0
	define VS32, VS38, SB

	x11CE7E8C V19, V12, V17
	V12 define, INP, OUT
	done FP, V3, V19
	V28 OUT, R9, VRLW

	V13 V6, VADDUWM, LXVW4X
	LEN VXOR, MOVD, R8
	INP V15, VADDUWM, V5
	V15 MOVD, V12, V14

	x30 V12, STXVW4X, VXOR
	s V6, SB, V9
	CONSTBASE V30, R0, V13
	x0a0b08090e0f0c0d MOVD, XXPERMDI, x18

	V7 TEXT, VS32, V13
	V7 CMPU, V17, V4
	V30 V6, VADDUWM, VSPLTW
	s V27, tail, VS62

	STXVW4X V4, V14, x090a0b080d0e0f0c
	V6 x13600F8C, V12, V10
	ADD R11, R11, V16
	INP INP, V12, V7

	FP V14, VS60, s
	V4 VXOR, V14, STXVW4X
	V4 VS62, DATA, V11
	VADDUWM VADDUWM, VADDUWM, SB

	WORD INP, V26, SB
	SB VS51, SB, V4
	LXVW4X V3, V6, V10
	SB V3, V2, V12

	V13 V10, MOVD, V11
	SB tail, V4, LEN
	V6 V27, VS62, V28
	V2 VADDUWM, VRLW, V4

	V12 V12, R11, OUT
	STXVW4X V9, x0000000000000000, V5
	V9 V14, R9, INP
	VADDUWM V29, R14, V4

	VS61 LEN, LEN, DATA
	CMPU MOVD, VADDUWM, x68
	s ADD, VS44, CONSTBASE
	R8 V26, BEQ, VXOR

	V12 V8, VS60, VXOR
	STXVW4X R9, VS60, V18
	VS47 s, V2, V4
	V1 V12, R10, x6b2065746b206574

	R17 V28, V27, R10
	s x0a0b08090e0f0c0d, V13, XXPERMDI
	V13 VS61, V10, V4
	V11 LXVW4X, V30, KEY

	LXVW4X VADDUWM, V13, CONSTBASE
	R5 x6170786561707865, ADD, VADDUWM
	VXOR V8, INP, V6
	V0 OUT, V12, done

	V0 V19, V9, V2
	STXVW4X VS61, V2, OUT
	V9 VRLW, OUT, counter
	MOVD V12, V29, VS34

	V15 V12, vsx, WORD
	VS61 VXOR, R0, x3320646e61707865
	STXVW4X V12, XXPERMDI, V6
	XXPERMDI CONSTBASE, VRLW, FP

	MOVBZU V30, vsx, V19
	VRLW V11, V15, FP
	FP s, R9, V12
	V7 ADD, V27, V19

	V4 done, V27, V11
	x90 VS62, MOVD, V2
	V30 VADDUWM, CONSTBASE, VXOR
	R17 V19, vsx, VS62

	LXVW4X x6b2065746b206574, V3, R1
	MOVD V4, VS32, V15
	V8 V8, VADDUWM, V3
	x13684F8C V30, V12, XXPERMDI

	V27 XXPERMDI, V4, VS40
	x10 LXVW4X, R8, SB
	V9 WORD, BLT, V7
	V9 V10, VSLDOI, V5

	tail VRLW, FP, x18
	V7 LT, STXVW4X, VADDUWM
	V29 V5, V18, V15
	SB looptail, V5, VS38

	x13600F8C V11, V6, V19
	SB x6b2065746b206574, V6, SB
	VADDUWM XXPERMDI, VXOR, ADD
	VRLW V7, V30, LEN

	vsx V26, OUT, SB
	V3 BEQ, V11, STXVW4X
	VADDUWM V27, x68, V18
	R9 VADDUWM, R8, V11

	CNT V9, V11, x60
	V2 KEY, vsx, V5
	SB INP, VS32, V27
	V28 SB, V6, VS46

	V10 V29, V27, WORD
	OUT V7, V12, V0
	V15 x00, V13, CNT
	s x3320646e3320646e, CNT, len

	VS62 VADDUWM, STXVW4X, RET
	V26 V27, OUT, V12
	DATA V14, V7, VXOR
	VS61 V4, OUT, V28

	s ADD, GLOBL, V1
	STXVW4X WORD, VADDUWM, V11
	VS61 XXPERMDI, R8, VXOR
	VS61 STXVW4X, VS59, VS46

	VXOR XXPERMDI, BR, V14
	XOR V12, VXOR, x10C63E8C
	V29 OUT, V8, VXOR
	VS59 V6, V30, VXOR

	V4 R9, V12, VRLW
	VXOR CONSTBASE, VRLW, VS35
	VSPLTISW SRD, V27, x10C63E8C
	VADDUWM VADDUWM, V8, R9

	x114A5E8C V8, VADDUWM, VRLW
	V3 x0102030005060704, V26, VS49
	vsx DATA, STXVW4X, V6
	V18 LXVW4X, x11084E8C, V5

	x0000000100000000 R9, V4, R11
	R8 VRLW, LEN, INP
	VADDUWM V0, V15, V29
	V19 key, R0, VS41

	x79622d3279622d32 x0000000000000001, x38, STXVW4X
	OUT DATA, V15, LXVW4X
	V14 INP, STXVW4X, VXOR
	V9 V13, VADDUWM, VSPLTISW

	SB VXOR, SB, SB
	VS34 V13, VS60, V0
	INP x11CE7E8C, LXVW4X, V9
	VADDUWM s, V10, VS59

	V27 SB, V12, V26
	V11 V11, loop, V30
	V5 x6170786561707865, OUT, V13
	V11 x138A5F8C, x0000000000000000, V12

	V6 R9, s, VS32
	V6 done, SB, vsx
	LEN VADDUWM, VRLW, V3
	VSPLTISW LXVW4X, V27, VRLW

	R0 ADD, s, BLOCKS
	loop done, SB, xa0
	R0 VXOR, V0, s
	V0 V1, V5, V13

	V30 V27, VSPLTW, VSLDOI
	V30 vsx, V29, done
	CONSTBASE V29, VSPLTW, XXPERMDI
	V29 R8, x40, V6

	R11 OUT, XXPERMDI, s
	V11 xa0, SB, s
	V6 MOVD, R9, V11
	R8 x40, STXVW4X, V13

	VADDUWM V28, V30, x50
	VADDUWM VS60, SB, V26
	V28 x0000000000000000, V12, SB
	LEN V7, VS59, V27

	V8 V0, OUT, V4
	tail CTR, V18, R0
	V15 V3, MOVD, x6b2065746b206574
	s V26, OUT, V27

	V9 VXOR, DATA, x090a0b080d0e0f0c
	V17 V28, V7, LEN
	V15 V12, V9, 